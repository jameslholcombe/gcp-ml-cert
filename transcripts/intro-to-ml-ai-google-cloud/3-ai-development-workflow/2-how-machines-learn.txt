To understand machine learning, you must first understand how neural networks learn.
00:04
This includes exploring this learning process and the terms associated with it.
00:10
How do machines learn?
00:11
And how do they assess their learning?
00:14
Before you dive into building an ML model, let's take a look at how a neural network learns.
00:20
You may already know about various neural networks, such as deep neural networks (or DNN), convolutional
00:26
neural networks (or CNN), recurrent neural networks (or RNN), and more recently large language models (LLMs).
00:36
These networks are used to solve different problems.
00:39
All of these models stem from the most basic: artificial neural network (or ANN).
00:45
ANNs are also referred to as neural networks or shallow neural networks.
00:50
Let’s focus on ANN to see how a neural network learns.
00:54
An ANN has three layers: an input layer, a hidden layer, and an output layer.
01:00
Each node represents a neuron.
01:01
The lines between neurons stimulate synopses, which is how information is transmitted in a human brain.
01:08
For instance, if you input article titles from multiple resources, the neural network can tell you
01:13
which media outlet or platform the article belongs to, such as GitHub, New York Times, and TechCrunch.
01:20
How does an ANN learn from examples and then make predictions?
01:24
Let’s examine how it works in depth.
01:26
Let’s assume you have two input neurons or nodes, one hidden neuron, and one output neuron.
01:32
Above the link between neurons are weights.
01:34
The weights retain information that a neural network learned through the training process and are the mysteries that a neural network aims to discover.
01:41
The first step is to calculate the weighted sum.
01:44
This is done by multiplying each input value by its corresponding weight, and then summing the products.
01:49
It normally includes a bias component bi.
01:53
However, to focus on the core idea, ignore it for now.
01:57
The second step is to apply an activation function to the weighted sum.
02:01
What is an activation function, and why do you need it?
02:04
Let’s pause your curiosity for just a moment and get back to that soon.
02:08
In the third step, the weighted sum is calculated for the output layer, assuming multiple neurons in the hidden layers.
02:15
The fourth step is to apply an activation function to the weighted sum.
02:19
This activation function can be different from the one applied to the hidden layers.
02:23
The result is the predicted y, which consists of the output layer.
02:27
You use y hat to represent the predicted result and y as the actual result.
02:34
Now let’s get back to activation functions.
02:36
What does an activation function do?
02:38
Well, an activation function is used to prevent linearity or add non-linearity.
02:45
What does that mean?
02:46
Think about a neural network.
02:49
Without activation functions, the predicted result y hat will always be a linear function of the input x, regardless of the number of layers between input and output.
02:58
Let’s walk through this for clarity.
03:01
Without the activation function, the value of the hidden layer h equals a total of w1 times x1 and w2 times x2.
03:12
Please note that to make this illustration easy, we ignored the bias component b, which you often see in other ML materials.
03:19
The output y hat therefore equals to w3 times h, and eventually equals to a total
03:25
of constant number a times x1 and a constant number b times x2 In other words,
03:31
the output Y is a linear combination of the input X. If y is a linear
03:38
function of x, you don’t need all the hidden layers, but only one input and one output!
03:44
You might already know that linear models do not perform well when handling comprehensive problems.
03:49
That’s why you must use activation functions to convert a linear network to a non-linear one.
03:56
What are the widely used activation functions?
03:59
You can use the rectified linear unit (or ReLU) function, which turns an input value to zero if it’s negative, or keeps the original value if it’s positive.
04:08
You can use the sigmoid function, which turns the input to a value between 0 and 1.
04:13
And hyperbolic tangent (Tanh) function, which shifts the sigmoid curve and generates a value between -1 and +1.
04:23
Another interesting and important activation function is called softmax.
04:27
Think about sigmoid: it generates a value from zero to one and is used for binary classification in logistic regression models.
04:35
An example for this would be deciding whether an email is spam.
04:39
What if you have multiple categories, such as GitHub, NYTimes, and TechCrunch?
04:44
Here you must use softmax, which is the activation function for multi-class classification.
04:51
It maps each output to a [0,1] range in a way that the total adds up to 1.
04:57
Therefore, the output of softmax is a probability distribution.
05:02
Skipping the math, you can conclude that softmax is used for multi-class classification, whereas sigmoid is used for binary-class classification in logistic regression models.
05:15
Also note that you don’t need to have the same activation function across different layers.
05:20
For instance, you can have ReLU for hidden layers and softmax for the output layer.
05:25
Now that you understand the activation function and get a predicted y, how do you know if the result is correct?
05:33
You use an assessment called loss function or cost function to measure the difference between the predicted y and the actual y.
05:40
Loss function is used to calculate errors for a single training instance, whereas cost function is used to calculate errors from the entire training set.
05:50
Therefore, in step five, you calculate the cost function to minimize the difference.
05:55
If the difference is significant, the neural network knows that it did a bad job in predicting and must go back to learn more and adjust parameters.
06:03
Many different cost functions are used in practice.
06:05
For regression problems, mean squared error, or MSE, is a common one used in linear regression models.
06:13
MSE equals the average of the sum of squared differences between y hat and y.
06:17
For classification problems, cross-entropy is typically used to measure the difference between the predicted and actual probability distributions in logistic regression models.
06:28
If the difference between the predicted and actual results is significant, you must go back to adjust weights and biases to minimize the cost function.
06:37
This potential sixth step is called backpropagation.
06:42
The challenge now is how to adjust the weights.
06:44
The solution is slightly complex, but indeed the most interesting part of a neural network.
06:51
The idea is to take cost functions and turn them into a search strategy.
06:56
That’s where gradient descent comes in.
06:59
Gradient descent refers to the process of walking down the surface formed by the cost function and finding the bottom.
07:06
It turns out that the problem of finding the bottom can be divided into two different and important questions: The first is: which direction should you take?
07:17
The answer involves the derivative.
07:18
Let’s say you start from the top left.
07:21
You calculate the derivative of the cost function and find it’s negative.
07:26
This means the angle of the slope is negative and you are at the left side of the curve.
07:31
To get to the bottom, you must go down and right.
07:34
Then, at one point, you are on the right side of the curve, and you calculate the derivative again.
07:41
This time the value is positive, and you must slide again to the left.
07:47
You calculate the derivative of the cost function every time to decide which direction to take.
07:52
Repeat this process, according to gradient descent, and you will eventually reach the regional bottom.
07:58
The second question in finding the bottom is what size should the steps be?
08:05
The step size depends on the learning rate, which determines the learning speed of how fast you bounce around to reach the bottom.
08:11
Step size or “learning rate” is a hyperparameter that is set before training.
08:17
If the step size is too small, your training might take too long.
08:21
If the step size is too large, you might bounce from wall to wall or even bounce out of the curve entirely, without converging.
08:28
When step size is just right, you’re set.
08:31
The seventh, and last step, is iteration.
08:35
One complete pass of the training process from step1 to step 6 is called an epoch.
08:41
You can set the number of epochs as a hyperparameter in training.
08:47
Weights or parameters are adjusted until the cost function reaches its optimum.
08:52
You can tell that the cost function has reached its optimum when the value stops decreasing, even after many iterations.
08:59
This is how a neural network learns.
09:01
It iterates the learning by continuously adjusting weights to improve behavior until it reaches the best result.
09:07
This is similar to a human learning lessons from the past.
09:12
We have illustrated a simple example with two input neurons (nodes), one hidden neuron, and one output neuron.
09:20
In practice, you might have many neurons in each layer.
09:23
Regardless of the number of neurons in the input, hidden, and output layer, the fundamental process of how a neural network learns remains the same.
09:33
Learning about neural networks can be exciting, but also overwhelming with the large number of new terms.
09:38
Let’s take a moment to review them.
09:40
In a neural network, weights and biases are parameters, which are learned by the machine during training.
09:45
You have no control of the parameters except to set the initial values.
09:50
The number of layers and neurons, activation functions, learning rate, and epochs are hyperparameters, which are decided by a human before training.
09:58
The hyperparameters determine how a machine learns.
10:01
For example, the learning rate decides how fast a machine learns and the number of epochs defines how many times the learning interates.
10:09
Normally, data scientists choose the hyperparameters and experiment with them to find the optimum combination.
10:15
However, if you use tools like AutoML, it automatically selects the hyperparameters for you and saves you plenty of experiment time.
10:23
You also learned about cost or loss functions, which are used to measure the difference between the predicted and actual value.
10:29
They are used to minimize error and improve performance.
10:33
You use backpropagation to modify the weights and bias if the difference is significant, and gradient descent to decide how to tune the weights and bias and when to stop.
10:44
These terms are your best friends when building an ML model.
10:46
You’ll revisit them in upcoming lessons and labs.